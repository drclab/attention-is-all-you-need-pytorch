{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TEXT CLASSIFICATION WITH THE TORCHTEXT LIBRARY](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#text-classification-with-the-torchtext-library)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this tutorial, we will show how to use the torchtext library to build the dataset for the text classification analysis. Users will have the flexibility to\n",
    "1. Access to the raw data as an iterator\n",
    "2. Build data processing pipeline to convert the raw text strings into torch.Tensor that can be used to train the model\n",
    "3. Shuffle and iterate the data with torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Prepare data processing pipelines](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#prepare-data-processing-pipelines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[335]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 2, 30, 5297]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('10')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Generate data batch and iterator](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#generate-data-batch-and-iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_txt = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_txt)\n",
    "        offsets.append(processed_txt.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#define-the-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is composed of the nn.EmbeddingBag layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of “mean” computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.Additionally, since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtClassModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.EmbeddingBag(vocab_size,embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.5\n",
    "        self.embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self,text, offsets):\n",
    "        embed = self.embed(text, offsets)\n",
    "        return self.fc(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "num_classes = len(set([label for (label, text) in train_iter]))\n",
    "embed_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TxtClassModel(vocab_size, embed_dim, num_classes).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Define functions to train the model and evaluate results](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#define-functions-to-train-the-model-and-evaluate-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, txt, offsets) in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "        pred_label = model(txt, offsets)\n",
    "        loss = criterion(pred_label, label)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optim.step()\n",
    "        total_acc += (pred_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f}'.format(\n",
    "                epoch,\n",
    "                idx,\n",
    "                len(dataloader),\n",
    "                total_acc / total_count\n",
    "            ))\n",
    "\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count =0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, txt, offsets) in enumerate(dataloader):\n",
    "            pred_label = model(txt, offsets)\n",
    "            loss = criterion(pred_label,  label)\n",
    "            total_acc += (pred_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from  torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer = optim, step_size=1.0, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train =  int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset)-num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.989\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.989\n",
      "-----------------------------------------------------------\n",
      "end of epoch   1 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.989\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.989\n",
      "-----------------------------------------------------------\n",
      "end of epoch   2 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.989\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.989\n",
      "-----------------------------------------------------------\n",
      "end of epoch   3 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.989\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.988\n",
      "-----------------------------------------------------------\n",
      "end of epoch   4 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.988\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.990\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.990\n",
      "-----------------------------------------------------------\n",
      "end of epoch   5 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.988\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.990\n",
      "-----------------------------------------------------------\n",
      "end of epoch   6 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.990\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.988\n",
      "-----------------------------------------------------------\n",
      "end of epoch   7 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.988\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.989\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.989\n",
      "-----------------------------------------------------------\n",
      "end of epoch   8 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.989\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.989\n",
      "-----------------------------------------------------------\n",
      "end of epoch   9 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.989\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.989\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.989\n",
      "-----------------------------------------------------------\n",
      "end of epoch  10 with validation accuracy    0.902\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    train(train_dl)\n",
    "    accu_val = evaluate(valid_dl)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print('-'*59)\n",
    "    print('end of epoch {:3d} with validation accuracy {:8.3f}'.format(epoch, accu_val))\n",
    "    print('-'*59)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Evaluate the model with test dataset](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#evaluate-the-model-with-test-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_test = evaluate(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.900921052631579"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was enduring the season’s worst weather conditions on Sunday at The Open on his way to a closing 75 at Royal Portrush, which considering the wind and the rain was a respectable showing. Thursday’s first round at the WGC-FedEx St. Jude Invitational was another story. With temperatures in the mid-80s and hardly any wind, the Spaniard was 13 strokes better in a flawless round. Thanks to his best putting performance on the PGA Tour, Rahm finished with an 8-under 62 for a three-stroke lead, which was even more impressive considering he’d never played the front nine at TPC Southwind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tensor = torch.tensor(text_pipeline(ex_text_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(txt_tensor, torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(1).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
